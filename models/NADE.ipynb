{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azhgh22/FaceGeneration/blob/main/models/NADE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Download Data and set up env**"
      ],
      "metadata": {
        "id": "EYpaUdYmJzCk"
      },
      "id": "EYpaUdYmJzCk"
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install kaggle wandb onnx -Uq\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! mkdir ~/.kaggle\n",
        "!cp /content/drive/MyDrive/ColabNotebooks/kaggle_API_credentials/kaggle.json ~/.kaggle/kaggle.json\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c challenges-in-representation-learning-facial-expression-recognition-challenge\n",
        "! unzip challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
        "! rm challenges-in-representation-learning-facial-expression-recognition-challenge.zip\n",
        "! rm fer2013.tar.gz\n",
        "! rm icml_face_data.csv\n",
        "! pip install torchview\n",
        "! pip install wandb"
      ],
      "metadata": {
        "id": "NDUJci8l7NuF"
      },
      "id": "NDUJci8l7NuF",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imports**"
      ],
      "metadata": {
        "id": "3dOPd2McJ7HE"
      },
      "id": "3dOPd2McJ7HE"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Main PyTorch Library\n",
        "from torch import nn # Used for creating the layers and loss function\n",
        "from torch.optim import Adam # Adam Optimizer\n",
        "import torchvision.transforms as transforms # Transform function used to modify and preprocess all the images\n",
        "from torch.utils.data import Dataset, DataLoader # Dataset class and DataLoader for creating the objects\n",
        "from sklearn.preprocessing import LabelEncoder # Label Encoder to encode the classes from strings to numbers\n",
        "import matplotlib.pyplot as plt # Used for visualizing the images and plotting the training progress\n",
        "from PIL import Image # Used to read the images from the directory\n",
        "import pandas as pd # Used to read/create dataframes (csv) and process tabular data\n",
        "import numpy as np # preprocessing and numerical/mathematical operations\n",
        "import os # Used to read the images path from the directory\n",
        "import torch.nn.functional as F\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torchview import draw_graph\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\" # detect the GPU if any, if not use CPU, change cuda to mps if you have a mac\n",
        "print(\"Device available: \", device)\n",
        "\n",
        "seed = 42\n",
        "\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ],
      "metadata": {
        "id": "zfML6EO8Jxyl",
        "outputId": "2e3e23f7-7227-46a1-84c4-761251ac18a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "zfML6EO8Jxyl",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device available:  cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Test Split, 48x48 grayscale pictures of different type of emotions**"
      ],
      "metadata": {
        "id": "FceDcFB_Kuis"
      },
      "id": "FceDcFB_Kuis"
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"train.csv\")\n",
        "data.head()"
      ],
      "metadata": {
        "id": "i42Xe1v7Kcge",
        "outputId": "0339c8e1-11ba-4cd4-8c43-9d21361ea0dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "id": "i42Xe1v7Kcge",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   emotion                                             pixels\n",
              "0        0  70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...\n",
              "1        0  151 150 147 155 148 133 111 140 170 174 182 15...\n",
              "2        2  231 212 156 164 174 138 161 173 182 200 106 38...\n",
              "3        4  24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...\n",
              "4        6  4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e526439e-5b57-44d2-ab2b-a1547c1df698\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>emotion</th>\n",
              "      <th>pixels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>70 80 82 72 58 58 60 63 54 58 60 48 89 115 121...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>151 150 147 155 148 133 111 140 170 174 182 15...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>231 212 156 164 174 138 161 173 182 200 106 38...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>24 32 36 30 32 23 19 20 30 41 21 22 32 34 21 1...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>6</td>\n",
              "      <td>4 0 0 0 0 0 0 0 0 0 0 0 3 15 23 28 48 50 58 84...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e526439e-5b57-44d2-ab2b-a1547c1df698')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e526439e-5b57-44d2-ab2b-a1547c1df698 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e526439e-5b57-44d2-ab2b-a1547c1df698');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-f5213a7b-ff3b-425b-96f9-b168e9d53d44\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f5213a7b-ff3b-425b-96f9-b168e9d53d44')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-f5213a7b-ff3b-425b-96f9-b168e9d53d44 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data",
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 28709,\n  \"fields\": [\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 0,\n        \"max\": 6,\n        \"num_unique_values\": 7,\n        \"samples\": [\n          0,\n          2,\n          5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pixels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 27473,\n        \"samples\": [\n          \"165 165 164 140 83 23 18 19 14 17 10 20 32 28 24 40 62 75 78 85 53 41 60 70 77 96 102 112 103 127 156 165 177 163 168 142 130 119 70 95 105 64 39 41 54 51 33 28 164 163 157 138 72 21 18 15 13 12 15 21 41 41 37 59 79 86 92 93 57 40 63 80 104 122 106 107 110 138 164 183 179 173 172 141 131 127 85 52 67 72 64 60 50 56 42 28 167 157 163 133 36 19 13 12 13 11 15 25 50 51 58 80 97 105 96 92 62 40 69 110 131 133 109 98 119 151 179 188 181 173 164 145 133 127 95 55 55 67 67 67 54 61 52 37 167 157 164 109 20 17 14 14 14 13 18 23 44 59 80 98 110 104 84 90 80 61 97 136 143 142 109 98 128 157 180 183 178 171 159 144 144 130 97 66 55 61 80 65 57 61 57 42 162 162 155 68 12 15 14 15 13 13 20 22 46 65 96 113 101 91 88 95 96 98 125 150 151 144 108 107 141 168 182 184 179 176 160 141 141 132 108 83 58 54 77 65 47 52 56 44 157 168 144 41 11 12 15 16 11 13 14 29 61 69 112 116 91 98 109 114 121 127 144 160 163 156 116 112 147 179 183 186 176 171 164 148 146 137 114 98 67 54 68 57 37 34 41 52 164 171 105 14 15 11 18 18 13 16 16 41 64 81 128 101 101 115 116 126 145 149 157 166 169 168 135 122 148 177 179 187 177 167 164 155 147 138 123 109 77 51 51 47 38 20 22 56 159 173 70 8 13 10 22 23 13 14 19 58 65 119 125 115 129 121 129 141 150 159 164 168 180 178 156 145 158 174 176 180 179 168 161 160 150 142 131 117 86 47 46 37 34 19 17 57 160 161 51 12 10 16 30 26 14 11 43 71 108 130 116 132 133 130 136 151 156 162 171 175 182 186 181 174 174 181 169 180 183 168 161 160 152 143 134 123 91 47 43 37 37 26 20 33 161 141 43 4 11 21 40 21 14 21 59 93 110 95 104 116 122 133 142 156 162 168 175 180 186 191 189 185 185 188 177 173 174 168 160 156 154 150 136 128 104 52 22 40 49 39 30 18 172 121 30 14 10 21 31 24 9 41 83 86 81 85 86 78 91 108 127 145 164 172 173 182 195 199 194 188 187 187 191 179 172 171 162 153 151 151 141 128 110 60 22 16 36 47 33 17 151 52 34 18 14 22 23 15 25 82 103 96 105 104 98 93 81 85 98 113 150 171 170 179 191 194 196 192 192 199 194 183 176 168 161 159 153 148 147 135 119 67 24 14 18 27 30 26 47 20 21 12 17 20 23 11 71 111 112 107 120 128 127 135 125 98 95 102 126 153 162 167 175 186 189 187 191 197 193 188 182 170 161 159 158 156 156 149 134 71 23 17 15 18 24 21 27 20 15 14 17 19 15 43 106 120 109 111 119 133 139 142 148 139 128 117 110 128 146 157 166 177 179 175 172 172 178 182 178 171 155 137 139 143 146 156 149 87 25 18 13 15 27 32 49 35 13 14 16 18 19 90 126 120 106 117 104 89 85 101 130 142 141 133 126 128 145 154 161 171 173 164 156 150 147 144 137 136 125 104 96 101 111 124 141 109 31 15 16 13 18 28 23 17 12 14 19 10 51 115 128 127 100 75 50 57 88 37 62 102 124 137 129 132 144 153 161 170 169 159 149 140 132 122 120 108 93 92 95 98 103 104 117 105 48 22 14 13 15 19 14 14 16 14 11 20 86 120 131 139 113 93 89 108 126 26 51 55 50 127 132 126 137 151 167 174 168 158 145 140 141 140 136 132 133 138 137 128 118 118 110 95 63 33 15 14 18 30 57 28 10 16 12 46 104 121 140 145 144 120 123 136 125 97 62 35 78 103 128 126 133 144 163 174 169 156 149 146 148 148 146 153 166 159 148 146 139 130 126 115 76 43 22 13 11 19 52 17 14 19 19 69 115 124 141 149 155 145 127 128 142 148 158 137 126 86 129 126 129 149 170 175 162 154 141 137 132 118 109 102 112 128 139 137 133 131 124 116 98 64 34 14 15 16 18 9 21 25 33 86 110 126 142 144 153 156 144 136 137 135 132 133 133 120 144 126 129 153 174 174 154 142 121 123 100 95 24 70 76 43 75 115 129 122 123 118 107 88 66 41 14 16 26 13 15 33 45 94 108 127 141 149 155 161 161 151 151 153 142 141 140 145 141 126 135 158 174 165 147 127 140 109 90 127 43 38 38 54 74 55 94 123 121 122 114 101 90 67 21 18 24 11 22 35 58 96 112 128 137 150 160 165 167 163 164 168 164 163 156 154 136 130 145 164 171 157 139 137 156 117 122 133 119 99 96 123 125 79 72 95 112 121 123 114 102 85 36 15 23 16 22 41 74 94 113 127 138 153 158 162 168 172 172 170 171 166 154 144 138 137 153 170 169 154 142 144 156 150 149 137 140 149 151 136 133 125 117 108 114 130 132 123 106 96 55 15 31 21 17 42 80 92 113 130 144 156 160 171 177 173 169 167 167 154 146 145 140 143 157 168 168 155 151 151 159 162 157 156 143 135 132 136 135 129 138 144 138 139 139 128 107 99 67 15 28 19 18 38 82 91 112 132 145 157 162 173 175 171 169 165 157 151 143 144 140 144 164 170 165 155 158 154 159 172 167 165 170 166 159 156 152 150 149 150 146 143 139 129 111 97 70 23 23 16 13 29 83 91 107 130 144 156 162 167 167 167 165 159 153 145 139 139 136 146 167 168 158 151 158 156 160 176 180 174 173 176 174 169 162 159 160 156 149 145 137 125 109 92 65 28 18 15 14 30 90 91 103 125 139 149 153 155 155 153 149 140 130 122 124 130 134 150 164 159 155 151 154 157 159 172 179 174 172 174 173 172 169 167 163 157 151 147 137 124 102 90 55 22 18 16 13 29 89 91 99 122 132 139 140 139 135 131 132 129 126 130 130 132 143 159 165 152 148 143 143 155 167 175 179 179 175 172 171 171 172 168 161 154 149 146 135 122 96 86 55 31 17 18 21 50 101 92 93 111 122 130 128 126 121 121 127 121 117 141 140 130 150 179 185 162 147 136 133 140 161 178 182 182 182 178 176 176 174 167 156 150 143 141 129 113 90 77 62 53 14 29 78 110 104 107 91 97 112 120 118 118 130 143 140 118 94 122 136 124 137 158 175 158 145 141 141 134 142 158 174 183 182 182 180 178 174 166 155 150 139 132 126 105 83 68 67 90 26 74 134 104 102 120 114 108 112 117 119 127 140 147 146 138 95 57 66 85 109 121 133 135 141 146 148 142 127 130 145 167 177 179 179 176 168 161 152 147 135 128 119 96 78 55 101 113 53 122 104 64 112 126 132 125 125 129 135 134 132 135 137 135 129 126 101 73 74 84 85 90 114 136 141 125 128 142 123 138 163 169 169 163 159 155 147 141 129 121 111 91 68 75 131 97 128 123 39 54 110 126 137 136 134 138 135 129 124 128 134 134 139 143 148 156 134 103 97 87 66 76 94 115 145 148 130 116 135 152 155 151 150 148 138 131 119 111 103 84 61 109 119 127 119 58 10 62 110 125 136 144 139 140 140 135 127 119 126 137 142 142 153 174 174 155 161 177 149 134 134 151 161 158 159 133 111 132 144 141 140 139 130 121 111 103 97 76 72 99 108 125 56 19 23 53 104 128 135 145 138 144 148 111 75 57 68 79 93 100 120 155 166 167 166 161 158 153 157 153 155 154 155 149 123 106 122 136 129 130 123 113 100 106 102 72 79 108 124 100 31 19 19 48 100 126 138 144 130 140 144 105 44 49 47 53 62 62 67 92 131 154 161 159 146 158 149 146 138 140 151 141 129 116 110 117 119 118 103 96 107 118 100 79 114 128 132 67 23 23 13 30 86 122 141 148 133 140 140 135 102 105 128 123 127 98 95 94 92 88 86 88 102 121 135 144 145 142 138 135 130 128 116 111 114 109 106 113 128 126 97 89 123 100 67 20 19 15 16 8 66 116 134 147 137 137 135 136 108 103 152 187 169 150 178 197 163 162 109 96 73 59 71 83 102 120 126 128 136 132 127 126 124 122 125 133 134 130 81 30 36 70 23 7 22 12 22 13 47 104 128 143 137 138 139 137 102 93 103 160 179 181 215 223 198 223 177 196 134 126 85 60 55 55 44 117 151 136 139 140 138 140 138 136 135 122 47 3 7 74 34 28 33 15 20 20 23 82 116 135 133 141 141 136 107 75 86 100 116 138 190 206 212 213 200 212 183 198 165 159 139 103 95 130 142 146 150 149 143 139 142 142 137 95 19 4 20 58 23 28 44 12 13 15 13 58 103 127 132 134 138 137 120 84 72 86 99 104 121 131 141 144 146 144 151 137 129 116 104 119 141 145 146 149 152 150 141 145 148 141 128 56 5 7 25 49 25 9 68 51 33 26 20 32 88 117 130 131 133 132 126 108 87 74 86 98 125 136 129 164 144 100 89 83 76 84 112 137 145 146 151 144 147 143 142 151 147 135 95 18 11 10 35 93 42 16 94 84 63 46 27 19 60 105 124 127 131 126 124 119 112 108 94 86 105 125 122 136 119 88 84 88 95 118 138 152 152 148 142 143 146 145 146 146 138 114 46 6 17 10 28 76 23 21 100 90 80 62 34 15 30 88 117 120 126 126 124 118 116 125 124 116 102 94 98 93 102 107 115 115 122 133 143 147 149 142 138 145 139 143 147 137 121 74 8 13 17 25 64 56 28 9 64 65 65 50 31 25 13 53 100 117 125 127 129 125 122 124 127 135 133 130 130 131 130 126 126 124 134 141 144 140 143 139 139 136 136 141 135 116 87 38 7 12 14 19 73 77 26 8 52 19 23 38 35 18 12 20 71 108 123 131 134 127 135 136 143 144 144 142 138 135 130 129 127 128 136 138 139 139 139 138 133 136 136 131 114 84 59 21 9 12 14 10 23 25 9 14 103 55 29 23 21 17 12 9 33 83 113 124 131 133 140 139 145 156 164 160 155 150 142 136 133 133 138 138 136 138 137 132 132 130 125 109 78 61 56 12 6 11 14 12 9 7 13 12 98 44 21 21 16 16 15 13 13 50 98 119 126 136 141 148 155 161 166 164 159 156 149 142 140 140 140 138 136 135 134 129 130 123 105 74 55 66 53 6 10 10 12 11 8 12 12 9\",\n          \"95 91 40 4 7 7 7 7 6 6 7 18 23 7 4 5 9 18 33 41 47 54 58 61 63 66 68 74 80 84 83 80 77 66 56 44 32 23 21 15 11 6 7 6 5 5 6 15 99 73 14 5 7 6 5 5 5 6 7 22 20 4 6 7 12 26 41 46 51 57 61 62 65 68 72 81 88 91 91 91 93 90 80 67 47 27 21 16 11 9 8 6 5 4 5 21 91 39 7 7 6 6 4 5 5 6 8 25 21 5 8 9 17 33 44 48 53 56 61 65 67 73 78 85 93 97 100 98 99 101 99 91 68 40 24 17 14 11 8 6 5 4 5 20 67 27 7 7 6 5 4 5 6 6 11 28 24 6 9 11 23 39 47 52 54 58 62 66 68 73 80 91 98 103 106 105 103 104 100 95 77 53 31 22 12 8 7 6 5 5 6 18 50 25 3 7 6 6 6 4 4 6 11 28 29 7 11 16 30 44 49 53 55 59 63 67 71 76 84 94 102 108 109 108 108 108 105 100 77 57 38 22 15 7 8 7 4 4 8 19 51 19 4 5 6 6 6 6 5 6 11 28 33 11 13 17 34 47 49 53 56 58 64 68 73 79 86 94 105 109 108 107 110 112 110 99 82 61 42 26 14 7 7 7 6 4 7 15 55 12 6 7 7 6 6 6 4 6 7 25 34 12 14 19 38 48 50 52 54 65 65 71 76 80 87 96 104 109 108 107 110 111 111 104 86 69 46 30 17 9 7 7 6 5 8 11 44 8 7 7 6 5 5 5 4 3 5 20 34 13 13 18 38 47 50 53 55 69 68 72 77 82 88 94 102 103 99 106 109 109 109 108 100 87 59 35 20 13 9 7 6 6 7 12 30 9 6 6 5 5 5 4 5 4 4 17 34 14 13 18 36 45 48 55 62 65 67 72 78 82 89 94 97 97 96 104 107 107 106 111 108 103 91 63 34 17 10 9 9 9 7 9 28 5 4 5 4 4 5 4 5 4 4 14 33 14 13 19 36 46 49 55 59 64 68 72 76 79 84 92 96 96 99 103 106 104 107 113 111 111 112 102 68 33 11 7 8 10 8 8 16 5 4 5 4 4 5 6 6 5 4 13 32 13 13 20 37 47 49 53 58 64 67 69 75 78 81 87 94 96 100 103 102 102 104 109 111 109 111 113 99 68 24 6 4 5 6 7 7 5 5 4 4 5 5 6 6 5 5 12 31 13 14 23 37 47 50 53 56 61 65 69 73 77 80 86 90 94 96 97 94 98 106 112 110 108 112 112 109 95 55 12 6 7 7 7 5 5 4 4 5 5 7 7 6 8 7 14 25 12 15 26 39 46 47 52 55 60 63 64 68 74 79 82 86 85 80 70 61 57 68 87 97 102 107 109 110 104 89 34 6 7 7 6 5 5 4 4 4 5 6 6 5 10 7 15 23 13 16 28 39 44 46 48 54 58 60 61 63 63 58 54 46 38 32 36 54 74 89 102 102 102 108 111 113 108 103 61 12 6 6 6 5 5 5 5 5 4 5 6 6 10 5 16 18 12 17 27 36 41 43 46 52 57 58 57 54 41 29 25 19 23 45 63 79 89 103 107 111 114 116 120 119 112 108 82 22 6 7 7 5 6 6 5 5 4 7 8 8 10 7 20 9 6 9 15 26 36 40 44 52 57 56 58 54 41 40 39 40 47 46 51 61 68 78 93 98 102 111 122 122 118 113 98 36 5 7 6 7 7 4 4 5 4 11 10 13 11 12 21 7 6 7 8 11 23 33 40 50 60 66 69 74 63 47 38 28 22 19 20 22 44 66 65 72 88 106 120 125 123 115 108 56 6 8 8 6 6 5 4 5 4 11 10 18 11 20 17 9 9 9 11 12 15 21 31 43 62 78 87 79 45 32 22 13 10 9 4 8 25 38 40 52 78 99 118 128 127 119 112 76 12 4 5 7 6 5 5 5 5 13 13 18 15 22 9 5 5 6 5 6 9 12 22 36 62 89 105 84 38 21 20 20 30 23 5 7 55 74 50 76 101 103 117 130 125 118 111 91 24 2 6 6 5 5 4 4 10 14 18 17 22 18 6 5 4 5 5 6 6 7 14 31 60 94 116 101 60 31 25 21 34 40 25 17 48 81 91 112 116 111 121 131 124 115 110 96 38 12 11 5 5 5 4 4 12 15 20 20 27 11 7 8 6 5 5 13 12 12 17 30 53 90 119 105 78 56 40 27 27 29 35 45 62 89 101 109 111 116 126 129 122 113 108 97 49 45 23 5 5 6 4 8 16 19 19 26 25 8 6 7 7 9 10 18 16 21 21 28 51 82 108 108 94 71 47 36 31 33 42 56 73 84 97 106 117 120 124 127 120 115 107 101 64 57 49 5 5 6 6 13 17 19 23 29 19 11 7 7 6 10 14 14 13 17 20 28 48 74 101 111 102 93 78 58 50 46 50 60 69 79 93 106 114 119 122 122 117 113 107 100 85 81 67 5 5 6 13 19 19 23 27 15 17 14 9 8 7 9 11 12 16 23 22 29 49 71 99 116 113 110 102 84 78 72 68 69 71 81 98 109 115 118 119 117 110 106 107 100 93 96 83 7 7 12 19 20 23 30 14 7 17 13 10 9 10 12 16 21 25 26 26 29 49 71 98 114 118 120 110 95 88 83 82 78 82 86 99 109 114 115 111 110 107 105 105 99 95 97 92 9 12 17 18 21 29 19 5 5 14 13 12 13 13 16 23 27 29 28 25 30 50 72 94 108 115 122 114 98 88 87 85 87 87 93 98 106 108 109 108 106 106 106 105 100 97 92 46 7 11 14 19 25 17 8 5 4 9 13 12 12 14 19 27 30 30 29 26 33 52 68 91 106 116 125 114 92 86 89 91 92 90 92 97 101 102 103 104 105 106 106 105 99 98 63 0 9 12 22 23 17 10 6 4 4 6 13 12 14 17 23 30 31 29 27 27 38 54 62 89 109 118 115 111 74 74 81 89 94 93 93 96 96 99 100 101 104 105 105 104 98 97 38 0 13 19 27 25 14 6 4 4 3 4 10 13 14 18 24 29 29 27 26 29 45 56 66 95 124 127 124 123 80 60 70 79 90 93 95 97 98 99 102 102 105 107 108 105 101 92 19 1 9 12 14 12 5 4 4 4 4 3 5 11 14 19 25 28 29 25 23 27 44 56 67 96 104 104 117 123 82 60 55 67 84 92 94 96 98 103 105 105 105 108 108 105 102 81 9 4 7 6 5 5 5 5 4 4 4 3 3 7 13 18 23 28 30 26 24 24 28 45 61 95 83 67 85 99 88 81 67 59 74 88 95 95 99 102 105 106 109 109 108 104 100 64 2 5 7 6 6 6 4 5 4 4 4 4 3 5 12 16 21 27 30 28 27 24 21 29 42 65 82 104 115 105 94 93 95 82 71 80 93 96 101 105 107 107 110 111 110 105 97 43 0 5 7 6 5 5 4 5 4 5 4 4 4 4 8 14 21 26 28 27 27 25 23 25 38 33 39 78 94 93 93 95 98 97 90 80 92 99 104 109 110 111 110 109 108 104 89 21 0 4 7 7 6 7 5 4 4 3 4 4 5 4 7 13 21 24 25 25 25 24 24 29 46 48 40 64 86 87 90 93 93 93 96 91 86 99 105 113 115 115 111 109 104 98 73 9 4 6 6 7 6 6 5 4 5 4 5 5 5 5 6 11 21 24 24 24 24 23 23 27 44 49 44 64 87 94 102 108 107 100 95 90 81 96 105 112 115 111 108 106 101 97 54 6 4 6 7 7 5 5 5 4 5 5 5 5 5 4 8 17 19 23 24 23 24 24 23 21 24 32 44 52 54 61 66 67 73 65 63 80 75 92 105 111 109 108 107 102 99 83 47 10 3 4 6 7 7 7 5 5 5 6 5 4 5 4 10 19 16 24 25 22 21 18 15 14 14 16 22 28 31 31 33 37 35 46 83 90 77 81 100 106 105 106 105 98 90 68 46 11 4 4 6 7 7 7 5 5 5 4 4 4 5 7 5 12 19 24 25 24 21 14 12 12 13 16 20 27 38 48 62 74 75 98 107 101 90 84 96 101 102 103 101 91 83 65 48 14 3 4 6 7 7 7 5 4 5 5 4 5 6 4 17 28 15 19 24 23 23 21 18 18 18 26 40 43 50 66 70 80 95 104 103 99 98 92 97 98 99 100 94 85 81 65 56 25 2 5 7 7 7 7 5 5 5 5 4 6 6 9 14 10 3 8 23 23 23 23 21 20 19 21 30 37 45 58 71 83 92 96 97 98 100 96 100 99 100 96 85 83 80 66 61 35 2 5 6 7 7 7 5 5 5 6 5 6 7 14 8 6 5 3 13 24 24 23 23 23 23 30 31 36 38 44 61 75 86 93 97 104 103 100 100 97 93 83 81 83 82 73 64 46 6 5 6 6 6 6 4 3 4 5 5 6 7 6 7 7 6 4 3 15 24 22 23 24 25 31 34 38 45 53 64 77 87 92 98 104 103 99 98 89 79 78 82 85 84 78 69 61 17 4 6 6 6 6 5 5 5 5 7 6 6 7 6 6 7 5 0 24 28 20 23 24 26 31 39 45 54 66 75 85 92 94 101 101 101 98 86 70 70 75 79 86 86 82 78 69 38 11 7 6 6 6 6 4 13 21 13 12 10 6 6 7 7 6 21 32 12 17 24 24 26 30 37 45 53 65 75 84 93 99 103 101 98 82 67 67 70 70 77 83 87 87 84 76 57 40 5 6 5 5 6 4 52 34 16 25 20 16 5 11 7 8 17 6 4 5 19 24 25 28 37 44 49 61 74 83 91 97 98 98 80 66 69 70 71 69 73 82 87 90 87 83 68 63 6 6 5 7 11 14 39 14 22 36 34 19 14 17 6 6 4 5 5 4 8 21 24 26 33 43 44 51 66 80 88 90 90 74 67 74 74 72 73 70 73 80 87 89 89 87 76 74 7 6 14 23 37 46 16 13 35 33 16 9 12 7 5 5 6 5 4 6 8 13 21 23 26 34 38 43 51 67 74 67 54 59 71 76 75 71 69 69 71 78 86 90 89 85 81 82 8 8 20 42 51 35 21 27 28 18 7 6 5 6 5 4 5 4 5 8 12 13 16 20 23 26 29 30 36 39 35 31 42 59 69 74 73 70 67 65 69 76 83 90 89 85 84 82\",\n          \"38 36 34 30 18 17 16 17 17 14 17 15 29 55 64 67 75 80 88 93 100 99 96 88 89 89 84 87 89 88 91 92 93 94 88 83 77 62 36 19 17 24 16 14 16 17 17 14 34 33 32 21 14 15 14 13 11 12 13 14 34 60 69 67 76 84 87 95 95 94 93 93 90 90 89 88 90 90 91 93 92 93 92 83 82 73 50 22 17 19 20 12 13 13 15 12 34 33 27 13 12 13 13 12 10 12 13 16 42 63 69 68 75 84 85 83 83 78 83 88 90 90 91 90 90 89 89 86 85 84 90 85 81 76 62 31 14 16 18 10 11 9 12 11 33 31 20 11 11 13 13 12 11 13 12 21 51 67 60 51 56 59 50 45 42 45 59 71 84 88 84 88 88 83 81 76 69 60 65 71 75 73 65 43 15 12 13 7 12 10 10 10 32 27 13 11 12 12 12 10 13 13 13 31 48 46 31 32 37 27 18 24 25 28 35 50 76 87 84 83 85 82 74 60 44 34 29 35 44 49 52 45 22 10 10 7 9 11 8 8 31 23 10 11 11 11 10 9 12 13 17 31 33 25 22 27 22 16 15 18 23 28 31 46 64 80 88 86 86 76 64 45 27 17 15 14 13 16 21 26 23 11 9 8 9 10 7 10 28 16 10 12 10 10 10 13 11 14 20 26 32 29 30 29 32 40 47 47 48 43 45 46 54 68 87 88 85 68 48 32 23 21 18 16 16 17 15 14 9 8 9 9 7 10 7 9 24 11 9 11 9 9 9 11 12 17 23 37 49 40 36 44 55 55 55 60 60 53 48 49 53 60 84 88 81 55 33 31 31 37 46 45 41 40 42 31 17 8 10 10 4 10 8 8 19 8 10 9 8 9 9 10 13 21 34 50 58 48 41 43 40 24 23 36 42 49 50 48 49 63 85 89 75 44 29 27 35 44 46 49 50 49 44 43 37 17 11 10 5 8 7 7 13 9 10 9 8 9 9 11 13 28 44 57 61 47 30 20 15 16 18 14 32 38 55 49 45 65 79 80 68 39 28 38 42 33 20 22 23 33 38 36 35 25 12 8 7 5 7 6 10 10 10 9 8 9 9 8 15 40 52 59 55 43 26 38 37 18 15 24 68 56 51 51 46 66 77 75 64 37 33 38 36 39 15 19 10 13 18 20 27 31 15 8 8 4 7 6 9 10 8 8 9 9 8 7 30 54 59 61 60 59 46 55 65 51 41 46 55 48 51 50 48 63 76 76 64 39 33 34 56 65 22 14 12 32 29 13 25 33 19 10 8 4 6 6 7 9 9 7 8 10 7 12 43 53 63 68 70 71 58 53 64 72 68 62 55 43 41 45 48 57 65 65 59 39 35 42 42 51 39 25 30 52 45 25 31 33 20 12 9 7 6 6 8 9 9 6 14 59 51 39 66 59 58 74 75 73 69 60 59 60 62 63 56 51 43 44 48 57 61 64 56 38 31 38 41 49 49 55 61 60 43 39 45 36 24 15 10 8 7 6 9 8 8 3 37 78 69 66 83 71 52 68 76 71 65 67 70 66 63 64 62 60 50 48 48 51 62 66 52 38 35 39 44 53 57 59 60 54 51 56 56 44 30 19 10 8 7 6 8 9 7 1 39 75 62 67 74 78 61 63 78 67 71 78 75 72 67 66 66 58 52 47 48 50 65 67 48 38 37 43 49 55 65 68 66 63 65 67 62 52 32 22 11 7 6 5 8 9 7 2 31 76 69 58 81 89 64 62 72 77 73 77 84 80 77 73 65 59 49 49 47 53 70 68 51 37 37 41 54 61 66 70 71 70 68 65 65 60 38 22 10 5 6 5 9 9 8 4 26 81 72 61 94 93 63 56 71 84 82 62 80 85 82 77 68 60 55 48 47 60 74 73 60 40 38 43 55 68 74 71 72 71 68 66 68 61 43 23 10 3 6 5 9 9 8 2 30 87 72 64 99 94 64 55 69 78 86 68 67 82 80 78 73 67 52 44 52 68 77 78 64 44 37 46 55 68 73 74 76 73 70 68 66 61 43 23 10 2 5 4 9 9 7 2 26 83 72 61 95 99 64 54 69 79 89 72 59 81 80 79 83 70 47 49 61 83 86 86 74 45 35 46 61 66 67 79 81 75 72 72 65 59 46 25 8 1 4 3 10 9 6 3 18 77 76 62 98 103 60 48 68 84 94 75 58 81 83 86 88 63 52 56 55 79 98 87 73 43 34 47 66 67 69 76 79 74 69 68 60 55 44 23 4 0 3 3 10 9 5 4 16 82 81 71 104 104 63 39 64 86 95 77 57 81 87 89 82 62 60 47 37 65 82 74 53 34 44 48 68 73 76 78 78 77 73 65 59 56 42 20 2 1 2 3 10 9 7 4 19 86 85 77 107 103 72 32 63 96 102 76 52 83 91 92 79 62 55 27 11 51 68 62 23 19 51 48 72 78 78 79 77 76 72 66 59 50 36 13 0 2 2 2 10 9 7 3 24 88 81 77 107 104 70 30 77 102 99 76 56 86 94 89 83 69 48 52 45 46 58 45 16 33 42 50 78 83 78 80 77 70 65 64 55 44 31 7 1 2 2 2 10 9 6 4 24 86 74 70 102 101 67 39 86 100 101 78 59 88 87 85 85 77 59 53 57 53 41 35 43 46 43 61 78 87 82 81 77 69 64 60 52 45 27 3 1 2 2 2 9 8 7 5 19 82 71 63 101 102 65 49 80 99 102 77 55 87 77 84 83 78 75 64 63 59 53 61 50 49 60 66 76 84 84 79 75 68 61 56 49 42 20 2 3 2 2 4 9 8 6 6 16 77 67 71 96 104 66 56 78 97 105 75 52 76 78 82 81 80 75 69 68 73 75 75 55 58 68 70 73 78 83 79 70 66 58 52 47 32 15 3 4 4 3 5 10 7 5 4 24 70 56 83 103 100 67 56 79 107 104 72 42 63 72 74 79 76 69 67 71 76 74 76 60 55 60 68 69 68 77 81 67 62 57 46 37 25 13 3 3 4 5 6 11 7 6 0 36 71 54 86 107 101 64 59 89 110 107 82 41 61 72 74 75 74 73 71 79 73 72 78 65 60 58 63 64 61 63 75 70 59 54 40 32 22 9 4 3 4 6 7 11 7 6 1 35 76 57 81 105 104 68 59 90 110 109 89 43 61 74 77 78 77 75 71 72 74 75 76 72 68 66 64 60 59 58 69 74 59 50 36 28 22 7 3 4 4 6 8 10 8 6 2 22 76 60 75 106 108 70 61 95 110 110 88 38 50 65 61 53 46 46 50 47 47 51 53 55 56 64 67 60 59 60 62 69 54 43 33 29 23 4 1 5 7 7 9 9 7 5 3 15 70 65 75 108 108 73 63 94 106 109 83 38 41 40 34 32 34 37 39 39 35 33 36 34 32 35 45 52 52 60 59 64 52 38 31 28 20 4 2 6 8 9 10 8 5 4 3 15 70 65 71 108 110 76 61 90 101 104 78 38 48 57 57 54 53 57 58 56 56 57 54 46 38 33 26 26 37 58 64 64 51 38 30 27 18 2 2 6 10 9 11 7 5 4 3 17 67 70 64 99 108 80 59 86 102 99 89 61 42 59 70 70 72 68 63 61 60 60 59 55 49 50 44 43 55 71 68 63 48 36 29 27 14 1 3 7 11 9 10 7 5 3 3 13 62 75 59 93 106 81 58 83 97 103 100 78 56 50 61 63 63 61 57 48 48 49 53 55 57 54 51 60 69 75 69 62 48 32 28 27 9 0 3 8 10 8 10 8 5 3 3 6 57 77 59 84 96 74 58 78 94 107 103 87 78 56 44 56 56 54 52 52 50 49 50 49 52 52 56 60 66 68 63 59 46 29 29 23 5 0 4 8 8 8 9 8 5 4 4 3 50 75 60 81 88 75 64 79 95 105 102 99 93 72 42 47 58 66 73 78 76 71 64 57 61 60 59 60 58 60 55 52 38 29 31 16 0 2 6 9 8 8 9 8 6 5 5 2 43 72 57 79 88 74 69 85 98 106 106 104 96 83 81 67 62 68 75 81 81 77 74 70 66 61 57 59 54 52 50 47 33 31 28 8 0 2 7 9 9 9 8 8 7 5 6 2 40 66 56 78 84 68 67 82 97 99 103 101 99 99 97 89 76 63 70 80 79 76 74 68 73 69 64 58 57 51 47 38 30 30 18 4 1 4 8 10 10 11 6 8 7 5 6 2 37 62 62 79 82 65 64 71 87 92 98 96 99 102 104 95 88 78 69 77 84 84 76 65 66 76 66 63 60 51 43 31 29 24 7 3 1 7 9 11 10 12 6 7 6 5 6 2 39 61 65 84 84 70 61 58 70 86 88 92 99 101 103 103 97 89 80 69 70 79 74 67 58 59 64 65 59 52 39 28 25 12 6 4 3 9 9 10 13 11 8 7 6 5 6 3 41 61 66 88 88 75 67 57 55 72 81 89 96 102 103 104 105 100 91 81 68 60 59 65 55 52 59 60 53 45 31 24 13 7 7 4 5 10 9 10 14 10 8 8 7 5 4 4 47 62 54 83 84 78 74 72 58 60 75 83 91 97 100 103 109 106 99 89 83 74 59 48 47 45 52 55 43 31 22 11 10 10 8 4 6 9 9 10 13 8 8 8 7 5 4 5 53 73 43 56 73 75 71 71 67 62 67 76 84 91 98 102 105 107 103 98 90 86 81 56 37 40 49 45 30 18 12 10 13 10 8 4 7 9 9 11 14 7 9 8 7 5 4 5 49 77 55 35 47 59 63 71 65 68 68 69 81 86 92 98 102 104 104 103 97 90 89 81 48 35 44 30 18 11 7 13 14 11 7 5 7 12 11 10 10 7 10 7 7 6 4 2 32 70 68 50 40 40 44 56 61 64 72 66 71 83 87 89 95 98 100 102 102 99 96 90 76 42 25 20 13 11 11 14 13 9 6 4 8 12 11 9 7 7 9 8 8 7 6 8 13 42 55 54 48 47 48 44 55 57 57 63 64 74 81 81 97 97 97 104 104 99 97 93 88 69 26 15 13 12 14 14 14 10 3 4 9 10 10 8 19 15 8 12 13 12 13 15 16 26 36 47 42 38 42 37 36 51 57 60 61 65 72 79 95 97 97 99 105 110 105 107 94 82 58 25 10 14 11 20 14 7 11 11 18 16 9 5 20 19 15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train, X_test = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val, test = train_test_split(X_test,test_size=0.5,random_state=42)\n",
        "\n",
        "pixel_arrays = np.stack(train['pixels'].apply(lambda x: np.fromstring(x, sep=' ')))\n",
        "train_tensor = torch.tensor(pixel_arrays, dtype=torch.float32).to(device)\n",
        "\n",
        "pixel_arrays = np.stack(val['pixels'].apply(lambda x: np.fromstring(x, sep=' ')))\n",
        "val_tensor = torch.tensor(pixel_arrays, dtype=torch.float32).to(device)\n",
        "\n",
        "pixel_arrays = np.stack(test['pixels'].apply(lambda x: np.fromstring(x, sep=' ')))\n",
        "test_tensor = torch.tensor(pixel_arrays, dtype=torch.float32).to(device)"
      ],
      "metadata": {
        "id": "r3Y_D6UoVytP"
      },
      "id": "r3Y_D6UoVytP",
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NADE model for x= {0,1..255}^n**"
      ],
      "metadata": {
        "id": "ddLhZ2TgW5AZ"
      },
      "id": "ddLhZ2TgW5AZ"
    },
    {
      "cell_type": "code",
      "source": [
        "class NADE256(nn.Module):\n",
        "  def __init__(self, input_dim=48*48, hidden_dim=512, n_values=256):\n",
        "    super().__init__()\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.n_values = n_values\n",
        "    self.model = torch.nn.Sequential(\n",
        "                  torch.nn.Linear(input_dim, hidden_dim),\n",
        "                  torch.nn.ReLU(),\n",
        "                  torch.nn.Linear(hidden_dim, n_values),\n",
        "                  torch.nn.LogSoftmax(dim=-1)\n",
        "                  ).to(device)\n",
        "\n",
        "  def forward(self, x):\n",
        "      \"\"\"\n",
        "      x: (batch_size, input_dim), each entry in [0, 255]\n",
        "      \"\"\"\n",
        "      x = x.to(device)\n",
        "      batch_size, input_dim = x.shape\n",
        "      losses = 0.0 # torch.tensor([0.0]*input_dim).to(device)\n",
        "\n",
        "      for i in range(self.input_dim):\n",
        "        mask = torch.tensor([1]*i + [0]*(self.input_dim-i)).to(device)\n",
        "        mask = mask.repeat(batch_size,1).to(device)\n",
        "        masked_input = (mask * x).to(device)\n",
        "\n",
        "        loss_i = self.model(masked_input)\n",
        "        # print(loss_i.shape)\n",
        "\n",
        "        target = x[:, i].long()\n",
        "        losses += F.nll_loss(loss_i, target, reduction='sum')\n",
        "\n",
        "      return losses\n"
      ],
      "metadata": {
        "id": "mXWiySTjqIzB"
      },
      "id": "mXWiySTjqIzB",
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = NADE256()\n",
        "model.forward(train_tensor[:2])[0].backward()"
      ],
      "metadata": {
        "id": "ozCti7km8bTT",
        "outputId": "5c67e44c-c8aa-4f10-ec85-2366ab59ab4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ozCti7km8bTT",
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 256])\n",
            "torch.Size([2, 256])\n",
            "torch.Size([2, 256])\n",
            "torch.Size([2, 256])\n",
            "torch.Size([2, 256])\n",
            "torch.Size([2, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NADE256()\n",
        "optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "EPOCHS = 10\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loss = 0.0\n",
        "\n",
        "for e in range(EPOCHS):\n",
        "  for i in range(0, len(train_tensor), BATCH_SIZE):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    batch = train_tensor[i:i+BATCH_SIZE].to(device)\n",
        "    loss = model.forward(batch)\n",
        "    train_loss += np.average(loss.detach().cpu().numpy())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  print(f\"Epoch {e} Train Loss: {train_loss}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "JGPZwimGEGWc",
        "outputId": "a30ab53d-8321-45ba-9e64-35410c4431bd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        }
      },
      "id": "JGPZwimGEGWc",
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 Train Loss: 690771072.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4073453805.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Genrating**"
      ],
      "metadata": {
        "id": "EJewflvULmCn"
      },
      "id": "EJewflvULmCn"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}